---
title: "DATA9001 Fundamentals of Data Science - Week 5 Tutorial Solutions"
author: "UNSW 2025 T2"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: flatly
    highlight: tango
  pdf_document:
    toc: true
    number_sections: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Setup

```{r packages}
# Install packages if needed
# install.packages("wooldridge")
# install.packages("lmtest")

library("wooldridge")
library("lmtest")
```

# Question 1

Use the data in `k401k` to estimate the model:
$$\text{prate} = \beta_0 + \beta_1 \text{mrate} + u$$

```{r q1-data}
data(k401k)
head(k401k)
```

## Part (i)

Find the average participation rate and the average match rate in the
sample of plans.

```{r q1-i}
mean(k401k$prate)
mean(k401k$mrate)
```

**Solution:** The average `prate` is about 87.36, and the average
`mrate` is about .732.

## Part (ii)

Now, estimate the simple regression equation
$\hat{\text{prate}} = \hat{\beta}_0 + \hat{\beta}_1 \text{mrate}$ and
report the results along with the sample size and R-squared.

```{r q1-ii}
lm_prate <- lm(prate~mrate, data = k401k)
summary(lm_prate)

b_0 <- lm_prate$coefficients[1]
b_1 <- lm_prate$coefficients[2]
b_0
b_1
```

**Solution:** The estimated equation is
$$\hat{\text{prate}} = 83.08 + 5.86 \text{ mrate}$$ n = 1,534, R² =
.075.

## Part (iii)

Interpret the intercept in your equation. Interpret the coefficient on
`mrate`.

**Solution:** The intercept implies that, even if `mrate` = 0, the
predicted participation rate is 83.08 percent. The coefficient on
`mrate` implies that a one-dollar increase in the match rate – a fairly
large increase – is estimated to increase `prate` by 5.86 percentage
points. This assumes, of course, that this change `prate` is possible
(if, say, `prate` is already at 98, this interpretation makes no sense).

## Part (iv)

Find the predicted `prate` when `mrate`=3.5. Is this a reasonable
prediction? Explain what is happening here.

```{r q1-iv}
mrate_value <- 3.5
predicted_particpation_rate <- b_0 + b_1*mrate_value
predicted_particpation_rate
predict(lm_prate, newdata = data.frame(mrate=3.5))
```

**Solution:** If we plug `mrate` = 3.5 into the equation, we get
$\hat{\text{prate}} = 83.08 + 5.86(3.5) = 103.59$. This is impossible,
as we can have at most a 100 percent participation rate. This
illustrates that, especially when dependent variables are bounded, a
simple regression model can give strange predictions for extreme values
of the independent variable. (In the sample of 1,534 firms, only 34 have
`mrate` ≥ 3.5.)

## Part (v)

How much of the variation in `prate` is explained by `mrate`? Is this a
lot in your opinion?

```{r q1-v}
summary(lm_prate)$r.squared
```

**Solution:** `mrate` explains about 7.5% of the variation in `prate`.
This is not much and suggests that many other factors influence 401(k)
plan participation rates.

# Question 2

Use the data in `hprice1` to estimate the model:
$$\text{price} = \beta_0 + \beta_1 \text{sqrft} + \beta_2 \text{bdrms} + u$$

where `price` is the house price measured in thousands of dollars.

```{r q2-data}
data(hprice1)
head(hprice1)
```

## Part (i)

Write out the results in equation form.

```{r q2-i}
lm_hp <- lm(price~sqrft+bdrms, data = hprice1)
summary(lm_hp)
```

**Solution:** The estimated equation is
$$\hat{\text{price}} = -19.32 + .128 \text{ sqrft} + 15.20 \text{ bdrms}$$
n = 88, R² = .632.

## Part (ii)

What is the estimated increase in price for a house with one more
bedroom, holding square footage constant?

**Solution:** Holding square footage constant,
$\Delta\hat{\text{price}} = 15.20 \Delta \text{bdrms}$, and so
$\hat{\text{price}}$ increases by 15.20, which means \$15,200.

## Part (iii)

What is the estimated increase in price for a house with an additional
bedroom that is 140 square feet in size? Compare this to your answer in
part (ii).

```{r q2-iii}
0.12844*140+15.19819
```

**Solution:** Now
$\Delta\hat{\text{price}} = .128\Delta \text{sqrft} + 15.20\Delta \text{bdrms} = .128(140) + 15.20 = 33.12$,
or \$33,120. Because the size of the house is increasing, this is a much
larger effect than in (ii).

## Part (iv)

What percentage of the variation in price is explained by square footage
and number of bedrooms?

```{r q2-iv}
summary(lm_hp)$r.squared
```

**Solution:** About 63.2%.

## Part (v)

The first house in the sample has `sqrft`=2,438 and `bdrms`=4. Find the
predicted selling price for this house from the OLS regression line.

```{r q2-v}
predict(lm_hp, newdata = data.frame(sqrft=2438,bdrms=4))
```

**Solution:** The predicted price is –19.32 + .128(2,438) + 15.20(4) =
353.544, or \$353,544.

## Part (vi)

The actual selling price of the first house in the sample was \$300,000
(so `price`=300). Find the residual for this house. Does it suggest that
the buyer underpaid or overpaid for the house?

```{r q2-vi}
300-predict(lm_hp, newdata = data.frame(sqrft=2438,bdrms=4))
```

**Solution:** From part (v), the estimated value of the home based only
on square footage and number of bedrooms is \$353,544. The actual
selling price was \$300,000, which suggests the buyer underpaid by some
margin. But, of course, there are many other features of a house (some
that we cannot even measure) that affect price, and we have not
controlled for these.

# Question 3

Use the data set in `wage2` for this problem. As usual, be sure all of
the following regressions contain an intercept.

```{r q3-data}
data(wage2)
head(wage2)
```

## Part (i)

Run a simple regression of `IQ` on `educ` to obtain the slope
coefficient, say, $\tilde{\delta}_1$.

```{r q3-i}
delta.til <- coef(lm(IQ~educ, data = wage2))[2]
delta.til
```

**Solution:** The slope coefficient from the regression `IQ` on `educ`
is (rounded to five decimal places) $\tilde{\delta}_1 = 3.53383$.

## Part (ii)

Run the simple regression of log(`wage`) on `educ`, and obtain the slope
coefficient, $\tilde{\beta}_1$.

```{r q3-ii}
beta1.til <- coef(lm(lwage~educ, data = wage2))[2]
beta1.til
```

**Solution:** The slope coefficient from log(`wage`) on `educ` is
$\tilde{\beta}_1 = .05984$.

## Part (iii)

Run the multiple regression of log(`wage`) on `educ` and `IQ`, and
obtain the slope coefficients, $\hat{\beta}_1$ and $\hat{\beta}_2$,
respectively.

```{r q3-iii}
beta1.hat <- coef(lm(lwage~educ+IQ, data = wage2))[2]
beta2.hat <- coef(lm(lwage~educ+IQ, data = wage2))[3]
beta1.hat
beta2.hat
```

**Solution:** The slope coefficients from log(`wage`) on `educ` and `IQ`
are $\hat{\beta}_1 = .03912$ and $\hat{\beta}_2 = .00586$, respectively.

## Part (iv)

Verify that
$\tilde{\beta}_1 = \hat{\beta}_1 + \hat{\beta}_2 \tilde{\delta}_1$.

```{r q3-iv}
beta1.hat+beta2.hat*delta.til
beta1.til
```

**Solution:** We have
$\hat{\beta}_1 + \tilde{\delta}_1\hat{\beta}_2 = .03912 + 3.53383(.00586) \approx .05983$,
which is very close to .05984; the small difference is due to rounding
error.

# Question 4

The following model can be used to study whether campaign expenditures
affect election outcomes:
$$\text{voteA} = \beta_0 + \beta_1 \log(\text{expendA}) + \beta_2 \log(\text{expendB}) + \beta_3 \text{prtystrA} + u$$

where `voteA` is the percentage of the vote received by Candidate A,
`expendA` and `expendB` are campaign expenditures by Candidates A and B,
and `prtystrA` is a measure of party strength for Candidate A (the
percentage of the most recent presidential vote that went to A's party).

```{r q4-data}
data(vote1)
```

## Part (i)

What is the interpretation of $\beta_1$?

**Solution:** Holding other factors fixed,
$$\Delta \text{voteA} = \beta_1\Delta \log(\text{expendA}) = \frac{\beta_1}{100} [100\Delta \log(\text{expendA})] \approx \frac{\beta_1}{100} (\%\Delta \text{expendA})$$

where we use the fact that
$100 \cdot \Delta \log(\text{expendA}) \approx \%\Delta \text{expendA}$.
So $\beta_1/100$ is the (ceteris paribus) percentage point change in
`voteA` when `expendA` increases by one percent.

## Part (ii)

In terms of the parameters, state the null hypothesis that a 1% increase
in A's expenditures is offset by a 1% increase in B's expenditures.

**Solution:** The null hypothesis is H₀: $\beta_1 = -\beta_2$, which
means a z% increase in expenditure by A and a z% increase in expenditure
by B leaves `voteA` unchanged. We can equivalently write H₀:
$\beta_1 + \beta_2 = 0$.

## Part (iii)

Estimate the given model using the data in `VOTE1` and report the
results in usual form. Do A's expenditures affect the outcome? What
about B's expenditures?

```{r q4-iii}
lm_vote <- lm(voteA~log(expendA)+log(expendB)+prtystrA, data = vote1)
summary(lm_vote)
```

**Solution:** The estimated equation (with standard errors in
parentheses below estimates) is

$$\hat{\text{voteA}} = 45.08 + 6.083 \log(\text{expendA}) - 6.615 \log(\text{expendB}) + .152 \text{ prtystrA}$$
$$(3.93) \quad (0.382) \quad\quad\quad\quad\quad (0.379) \quad\quad\quad\quad\quad (0.062)$$

n = 173, R² = .793.

The coefficient on log(`expendA`) is very significant (t statistic ≈
15.92), as is the coefficient on log(`expendB`) (t statistic ≈ –17.45).
The estimates imply that a 10% ceteris paribus increase in spending by
candidate A increases the predicted share of the vote going to A by
about .61 percentage points. Similarly, a 10% ceteris paribus increase
in spending by B reduces $\hat{\text{voteA}}$ by about .66 percentage
points. These effects certainly cannot be ignored.

While the coefficients on log(`expendA`) and log(`expendB`) are of
similar magnitudes (and opposite in sign, as we expect), we do not have
the standard error of $\hat{\beta}_1 + \hat{\beta}_2$, which is what we
would need to test the hypothesis from part (ii).

Can you use these results to test the hypothesis in part (ii)?

**Answer:** No, we cannot directly test the hypothesis because we need
the standard error of $\hat{\beta}_1 + \hat{\beta}_2$.

## Part (iv)

Estimate a model that directly gives the t statistic for testing the
hypothesis in part (ii). What do you conclude? (Use a two-sided
alternative.)

```{r q4-iv}
vote1$expend_diff <- log(vote1$expendB)-log(vote1$expendA)
lm_vote1 <- lm(voteA~log(expendA)+expend_diff+prtystrA, data = vote1)
summary(lm_vote1)
```

**Solution:** Write $\theta_1 = \beta_1 + \beta_2$, or
$\beta_1 = \theta_1 - \beta_2$. Plugging this into the original
equation, and rearranging, gives
$$\text{voteA} = \beta_0 + \theta_1\log(\text{expendA}) + \beta_2 [\log(\text{expendB}) - \log(\text{expendA})] + \beta_3 \text{prtystrA} + u$$

When we estimate this equation, we obtain $\hat{\theta}_1 \approx -.532$
and se($\hat{\theta}_1$) ≈ .533. The t statistic for the hypothesis in
part (ii) is -.532/.533 ≈ -1. Therefore, we fail to reject H₀:
$\beta_1 = -\beta_2$.

# Question 5

The file `jtrain2` contains data on a job training experiment for a
group of men. Men could enter the program starting in January 1976
through about mid-1977. The program ended in December 1977. The idea is
to test whether participation in the job training program had an effect
on unemployment probabilities and earnings in 1978.

```{r q5-data}
data(jtrain2)
head(jtrain2, 4)
```

## Part (i)

The variable `train` is the job training indicator. How many men in the
sample participated in the job training program? What was the highest
number of months a man actually participated in the program?

```{r q5-i}
sum(jtrain2$train)
max(jtrain2$mostrn)
```

**Solution:** 185 out of 445 participated in the job training program.
The longest time in the experiment was 24 months (obtained from the
variable `mosinex`).

## Part (ii)

Run a linear regression of `train` on several demographic and
pretraining variables: `unem74`, `unem75`, `age`, `educ`, `black`,
`hisp`, and `married`. Are these variables jointly significant at the 5%
level?

```{r q5-ii}
lm_train <- lm(train~unem74+unem75+age+educ+black+hisp+married, data = jtrain2)
summary(lm_train)
anova(lm_train)
```

**Solution:** The F statistic for joint significance of the explanatory
variables is F(7,437) = 1.43 with p-value = .19. Therefore, they are
jointly insignificant at even the 15% level. Note that, even though we
have estimated a linear probability model, the null hypothesis we are
testing is that all slope coefficients are zero, and so there is no
heteroskedasticity under H₀. This means that the usual F statistic is
asymptotically valid.

## Part (iii)

Estimate a probit version of the linear model in part (ii). Compute the
likelihood ratio test for joint significance of all variables. What do
you conclude?

```{r q5-iii}
lm_train1 <- glm(train~unem74+unem75+age+educ+black+hisp+married,
                 family = binomial(link = "probit"), data = jtrain2)
summary(lm_train1)
anova(lm_train1)
lrtest(lm_train1)
```

**Solution:** After estimating the model P(train = 1\|x) = Φ(b₀ +
b₁unem74 + b₂unem75 + b₃age + b₄educ + b₅black + b₆hisp + b₇married) by
probit maximum likelihood, the likelihood ratio test for joint
significance is 10.18. In a χ²₇ distribution this gives p-value = .18,
which is very similar to that obtained for the LPM in part (ii).

## Part (iv)

Based on your answers to parts (ii) and (iii), does it appear that
participation in job training can be treated as exogenous for explaining
1978 unemployment status? Explain.

**Solution:** Training eligibility was randomly assigned among the
participants, so it is not surprising that `train` appears to be
independent of other observed factors. (However, there can be a
difference between eligibility and actual participation, as men can
always refuse to participate if chosen.)

## Part (v)

Run a simple regression of `unem78` on `train` and report the results in
equation form. What is the estimated effect of participating in the job
training program on the probability of being unemployed in 1978? Is it
statistically significant?

```{r q5-v}
lm_train2 <- lm(unem78~train, data = jtrain2)
summary(lm_train2)
```

**Solution:** The simple LPM results are
$$\hat{\text{unem78}} = .354 - .111 \text{ train}$$
$$(.028) \quad (.044)$$

n = 445, R² = .014.

Participating in the job training program lowers the estimated
probability of being unemployed in 1978 by .111, or 11.1 percentage
points. This is a large effect: the probability of being unemployed
without participation is .354, and the training program reduces it to
.243. The differences is statistically significant at almost the 1%
level against at two-sided alternative. (Note that this is another case
where, because training was randomly assigned, we have confidence that
OLS is consistently estimating a causal effect, even though the
R-squared from the regression is very small. There is much about being
unemployed that we are not explaining, but we can be pretty confident
that this job training program was beneficial.)

## Part (vi)

Run a probit of `unem78` on `train`. Does it make sense to compare the
probit coefficient on `train` with the coefficient obtained from the
linear model in part (v)?

```{r q5-vi}
lm_train3 <- glm(unem78~train, family = binomial(link = "probit"), data = jtrain2)
summary(lm_train3)
```

**Solution:** The estimated probit model is
$$P(\text{unem78} = 1|\text{train}) = \Phi(-.375 - .321 \text{ train})$$
$$(.080) \quad (.128)$$

where standard errors are in parentheses. It does not make sense to
compare the coefficient on `train` for the probit, -.321, with the LPM
estimate. The probabilities have different functional forms.

However, note that the probit and LPM t statistics are essentially the
same (although the LPM standard errors should be made robust to
heteroskedasticity).

## Part (vii)

Find the fitted probabilities from parts (v) and (vi). Explain why they
are identical. Which approach would you use to measure the effect and
statistical significance of the job training program?

```{r q5-vii}
predict(lm_train2, newdata = data.frame(train = 0), type = "response")
predict(lm_train2, newdata = data.frame(train = 1), type = "response")
predict(lm_train3, newdata = data.frame(train = 0), type = "response")
predict(lm_train3, newdata = data.frame(train = 1), type = "response")
```

**Solution:** There are only two fitted values in each case, and they
are the same: .354 when `train` = 0 and .243 when `train` = 1. This has
to be the case, because any method simply delivers the cell frequencies
as the estimated probabilities. The LPM estimates are easier to
interpret because they do not involve the transformation by Φ(×), but it
does not matter which is used provided the probability differences are
calculated.

## Part (viii)

Add all of the variables from part (ii) as additional controls to the
models from parts (v) and (vi). Are the fitted probabilities now
identical? What is the correlation between them?

```{r q5-viii}
lm_train4 <- lm(unem78~train+unem74+unem75+age+educ+black+hisp+married, data = jtrain2)
lm_train5 <- glm(unem78~train+unem74+unem75+age+educ+black+hisp+married,
                 family = binomial(link = "probit"), data = jtrain2)
predict4 <- predict(lm_train4, type = "response")
predict5 <- predict(lm_train5, type = "response")
cor(predict4,predict5)
plot(predict4,predict5,col="red",lwd=5)
```

**Solution:** The fitted values are no longer identical because the
model is not saturated. That is, the explanatory variables are not an
exhaustive, mutually exclusive set of dummy variables. But, because the
other explanatory variables are insignificant, the fitted values are
highly correlated: the LPM and probit fitted values have a correlation
of about .993.

## Part (ix)

Using the model from part (viii), estimate the average partial effect of
`train` on the 1978 unemployment probability. How does the estimate
compare with the OLS estimate from part (viii)?

```{r q5-ix}
jtrain2c <- jtrain2
jtrain2c$train <- 1-jtrain2c$train
predict6 <- predict(lm_train5, newdata = jtrain2c, type = "response")
PE <- predict5-predict6
PE[jtrain2$train==0] <- -PE[jtrain2$train==0]
mean(PE)
summary(lm_train4)
```

**Solution:** To obtain the average partial effect of `train` using the
probit model, we obtain fitted probabilities for each man for `train` =
1 and `train` = 0. Of course, one of these is a counterfactual, because
the man was either in job training or not. Of course, we evaluate the
other regressors at their actual outcomes. The APE is the average, over
all observations, of the differences in the estimated probabilities.
With the variables in part (ii) appearing in the probit, the estimated
APE is about -.112. Interestingly, rounded to three decimal places, this
is the same as the coefficient on `train` in the linear regression. In
other words, the linear probability model and probit give virtually the
same estimated APEs.
